{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a51240db",
   "metadata": {},
   "source": [
    "# Unsupervised Sleep Stage Clustering with CNN+LSTM\n",
    "\n",
    "This notebook implements a deep learning pipeline to cluster sleep stages from PSG signals and identify micro arousals at millisecond scale in an unsupervised manner. We'll use:\n",
    "- PyTorch for building the neural network\n",
    "- CuPy for GPU acceleration\n",
    "- Dynamic Mode Decomposition (DMD) for dimension reduction"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f0146d4",
   "metadata": {},
   "source": [
    "## 1. Import Libraries and Setup GPU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "eedd04dc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using GPU: NVIDIA GeForce GTX 1080 Ti\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import cupy as cp\n",
    "from sklearn.cluster import KMeans, DBSCAN\n",
    "from sklearn.manifold import TSNE\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import pyedflib\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Check if GPU is available\n",
    "if not torch.cuda.is_available():\n",
    "    raise RuntimeError(\"GPU is required for this pipeline. CUDA not available.\")\n",
    "\n",
    "device = torch.device(\"cuda\")\n",
    "print(f\"Using GPU: {torch.cuda.get_device_name()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6dc63970",
   "metadata": {},
   "source": [
    "## 2. Data Loading and Preprocessing\n",
    "\n",
    "We'll start by defining functions to load and preprocess EDF files containing PSG data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "09f6c309",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PSGDataset(Dataset):\n",
    "    def __init__(self, file_paths, window_size=30, overlap=0.5, transform=None):\n",
    "        \"\"\"\n",
    "        Dataset for PSG signals\n",
    "        \n",
    "        Args:\n",
    "            file_paths: List of paths to EDF files\n",
    "            window_size: Size of the sliding window in seconds\n",
    "            overlap: Overlap between windows (0-1)\n",
    "            transform: Optional transform to apply to the data\n",
    "        \"\"\"\n",
    "        self.file_paths = file_paths\n",
    "        self.window_size = window_size\n",
    "        self.overlap = overlap\n",
    "        self.transform = transform\n",
    "        \n",
    "        self.signal_names = ['EEG Fpz-Cz', 'EEG Pz-Oz', 'EOG horizontal', \n",
    "                            'Resp oro-nasal', 'EMG submental', 'Temp rectal', 'Event marker']\n",
    "        self.sampling_rates = [100, 100, 100, 1, 1, 1, 1]  # Hz\n",
    "        \n",
    "        self.segments = self._prepare_segments()\n",
    "        \n",
    "    def _prepare_segments(self):\n",
    "        segments = []\n",
    "        \n",
    "        for file_path in self.file_paths:\n",
    "            signals, signal_headers = self._load_edf(file_path)\n",
    "            \n",
    "            # Create segments\n",
    "            for i, (signal, header, rate) in enumerate(zip(signals, signal_headers, self.sampling_rates)):\n",
    "                step = int(self.window_size * rate * (1 - self.overlap))\n",
    "                window_samples = int(self.window_size * rate)\n",
    "                \n",
    "                for start in range(0, len(signal) - window_samples, step):\n",
    "                    segment = signal[start:start+window_samples]\n",
    "                    segments.append({\n",
    "                        \"signal\": segment,\n",
    "                        \"signal_name\": header['label'],\n",
    "                        \"start_time\": start / rate,\n",
    "                        \"sampling_rate\": rate,\n",
    "                        \"file_path\": file_path\n",
    "                    })\n",
    "        \n",
    "        return segments\n",
    "    \n",
    "    def _load_edf(self, file_path):\n",
    "        f = pyedflib.EdfReader(file_path)\n",
    "        n = f.signals_in_file\n",
    "        signal_headers = f.getSignalHeaders()\n",
    "        signals = [f.readSignal(i) for i in range(n)]\n",
    "        f.close()\n",
    "        return signals, signal_headers\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.segments)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        segment = self.segments[idx]\n",
    "        data = segment[\"signal\"]\n",
    "        \n",
    "        if self.transform:\n",
    "            data = self.transform(data)\n",
    "        \n",
    "        # Convert to tensor\n",
    "        data = torch.FloatTensor(data)\n",
    "        \n",
    "        return data, segment[\"signal_name\"], segment[\"start_time\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0bd25988",
   "metadata": {},
   "source": [
    "## 3. Dynamic Mode Decomposition (DMD) for Feature Extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a319fd52",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DMD:\n",
    "    def __init__(self, rank=None, svd_rank=None):\n",
    "        \"\"\"\n",
    "        Dynamic Mode Decomposition for feature extraction\n",
    "        \n",
    "        Args:\n",
    "            rank: Number of DMD modes to compute\n",
    "            svd_rank: SVD rank to use for truncated SVD\n",
    "        \"\"\"\n",
    "        self.rank = rank\n",
    "        self.svd_rank = svd_rank\n",
    "        self.modes = None\n",
    "        self.dynamics = None\n",
    "        self.eigenvalues = None\n",
    "    \n",
    "    def fit_transform(self, X):\n",
    "        \"\"\"\n",
    "        Compute DMD modes and return the reconstructed data\n",
    "        \n",
    "        Args:\n",
    "            X: Data matrix (time steps x features)\n",
    "            \n",
    "        Returns:\n",
    "            Transformed data\n",
    "        \"\"\"\n",
    "        # Use CuPy for GPU acceleration\n",
    "        X_gpu = cp.asarray(X)\n",
    "        \n",
    "        # Split data into snapshots\n",
    "        X1 = X_gpu[:, :-1]\n",
    "        X2 = X_gpu[:, 1:]\n",
    "        \n",
    "        # SVD on X1\n",
    "        U, sigma, Vh = cp.linalg.svd(X1, full_matrices=False)\n",
    "        \n",
    "        # Truncate SVD if svd_rank is specified\n",
    "        if self.svd_rank is not None:\n",
    "            r = min(self.svd_rank, len(sigma))\n",
    "        elif self.rank is not None:\n",
    "            r = min(self.rank, len(sigma))\n",
    "        else:\n",
    "            # Use cumulative energy to determine rank\n",
    "            cumulative_energy = cp.cumsum(sigma**2) / cp.sum(sigma**2)\n",
    "            r = cp.argmax(cumulative_energy >= 0.99) + 1\n",
    "        \n",
    "        U = U[:, :r]\n",
    "        sigma = sigma[:r]\n",
    "        Vh = Vh[:r, :]\n",
    "        \n",
    "        # Compute A tilde (reduced order operator)\n",
    "        A_tilde = cp.dot(cp.dot(U.T, X2), cp.diag(1.0/sigma).dot(Vh))\n",
    "        \n",
    "        # Eigendecomposition of A tilde\n",
    "        eigenvalues, eigenvectors = cp.linalg.eig(A_tilde)\n",
    "        \n",
    "        # DMD modes\n",
    "        modes = cp.dot(cp.dot(X2, cp.diag(1.0/sigma).dot(Vh).T), eigenvectors)\n",
    "        \n",
    "        # Save results\n",
    "        self.modes = cp.asnumpy(modes)\n",
    "        self.eigenvalues = cp.asnumpy(eigenvalues)\n",
    "        \n",
    "        # Return DMD features\n",
    "        return cp.asnumpy(cp.abs(modes).T)\n",
    "    \n",
    "    def get_feature_importance(self):\n",
    "        \"\"\"\n",
    "        Get DMD mode importance based on eigenvalues\n",
    "        \n",
    "        Returns:\n",
    "            Mode importance scores\n",
    "        \"\"\"\n",
    "        if self.eigenvalues is None:\n",
    "            raise ValueError(\"DMD not fit yet. Call fit_transform first.\")\n",
    "        \n",
    "        # Importance is based on the modulus of eigenvalues\n",
    "        importance = np.abs(self.eigenvalues)\n",
    "        return importance / np.sum(importance)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6ff0bab",
   "metadata": {},
   "source": [
    "## 4. CNN+LSTM Model Architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "157af987",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CNN_LSTM(nn.Module):\n",
    "    def __init__(self, input_channels=7, hidden_dims=128, latent_dim=64):\n",
    "        \"\"\"\n",
    "        CNN+LSTM model for unsupervised learning of PSG signals\n",
    "        \n",
    "        Args:\n",
    "            input_channels: Number of input channels (signal types)\n",
    "            hidden_dims: Dimensions of hidden layers\n",
    "            latent_dim: Dimension of latent space\n",
    "        \"\"\"\n",
    "        super(CNN_LSTM, self).__init__()\n",
    "        \n",
    "        # CNN feature extractor\n",
    "        self.feature_extractor = nn.Sequential(\n",
    "            nn.Conv1d(input_channels, 64, kernel_size=5, stride=1, padding=2),\n",
    "            nn.BatchNorm1d(64),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool1d(2),\n",
    "            \n",
    "            nn.Conv1d(64, 128, kernel_size=5, stride=1, padding=2),\n",
    "            nn.BatchNorm1d(128),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool1d(2),\n",
    "            \n",
    "            nn.Conv1d(128, 256, kernel_size=5, stride=1, padding=2),\n",
    "            nn.BatchNorm1d(256),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool1d(2),\n",
    "        )\n",
    "        \n",
    "        # LSTM for temporal modeling\n",
    "        self.lstm = nn.LSTM(256, hidden_dims, num_layers=2, batch_first=True, dropout=0.2, bidirectional=True)\n",
    "        \n",
    "        # MLP for latent representation\n",
    "        self.mlp = nn.Sequential(\n",
    "            nn.Linear(hidden_dims*2, hidden_dims),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_dims, latent_dim)\n",
    "        )\n",
    "        \n",
    "        # Decoder for reconstruction (will be used in training)\n",
    "        self.decoder = nn.Sequential(\n",
    "            nn.Linear(latent_dim, hidden_dims),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_dims, input_channels * 100)  # Assuming 1 second of data at 100 Hz\n",
    "        )\n",
    "    \n",
    "    def forward(self, x):\n",
    "        # x shape: (batch_size, channels, time_steps)\n",
    "        \n",
    "        # Extract CNN features\n",
    "        cnn_features = self.feature_extractor(x)\n",
    "        \n",
    "        # Reshape for LSTM: (batch_size, time_steps, features)\n",
    "        cnn_features = cnn_features.permute(0, 2, 1)\n",
    "        \n",
    "        # Apply LSTM\n",
    "        lstm_out, _ = self.lstm(cnn_features)\n",
    "        \n",
    "        # Get final time step output\n",
    "        lstm_out = lstm_out[:, -1, :]\n",
    "        \n",
    "        # Get latent representation\n",
    "        latent = self.mlp(lstm_out)\n",
    "        \n",
    "        # Decode for reconstruction\n",
    "        reconstruction = self.decoder(latent)\n",
    "        reconstruction = reconstruction.view(x.size(0), x.size(1), -1)\n",
    "        \n",
    "        return latent, reconstruction\n",
    "    \n",
    "    def encode(self, x):\n",
    "        # Only return the latent representation\n",
    "        cnn_features = self.feature_extractor(x)\n",
    "        cnn_features = cnn_features.permute(0, 2, 1)\n",
    "        lstm_out, _ = self.lstm(cnn_features)\n",
    "        lstm_out = lstm_out[:, -1, :]\n",
    "        latent = self.mlp(lstm_out)\n",
    "        return latent"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "512b3a92",
   "metadata": {},
   "source": [
    "## 5. Unsupervised Training Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "30a375d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "class UnsupervisedTrainer:\n",
    "    def __init__(self, model, optimizer, scheduler=None, device='cuda'):\n",
    "        \"\"\"\n",
    "        Trainer for unsupervised learning\n",
    "        \n",
    "        Args:\n",
    "            model: CNN_LSTM model\n",
    "            optimizer: Optimizer\n",
    "            scheduler: Learning rate scheduler\n",
    "            device: Device to use\n",
    "        \"\"\"\n",
    "        self.model = model\n",
    "        self.optimizer = optimizer\n",
    "        self.scheduler = scheduler\n",
    "        self.device = device\n",
    "        self.reconstruction_loss = nn.MSELoss()\n",
    "        \n",
    "        # For contrastive learning\n",
    "        self.temperature = 0.5\n",
    "    \n",
    "    def train_epoch(self, dataloader, epoch):\n",
    "        self.model.train()\n",
    "        total_loss = 0\n",
    "        total_recon_loss = 0\n",
    "        total_contrastive_loss = 0\n",
    "        \n",
    "        for batch_idx, (data, signal_names, _) in enumerate(dataloader):\n",
    "            data = data.to(self.device)\n",
    "            \n",
    "            # Create augmented versions\n",
    "            data_aug1 = self._augment(data)\n",
    "            data_aug2 = self._augment(data)\n",
    "            \n",
    "            # Forward pass for original and augmented data\n",
    "            latent, reconstruction = self.model(data)\n",
    "            latent_aug1, _ = self.model(data_aug1)\n",
    "            latent_aug2, _ = self.model(data_aug2)\n",
    "            \n",
    "            # Reconstruction loss\n",
    "            recon_loss = self.reconstruction_loss(reconstruction, data)\n",
    "            \n",
    "            # Contrastive loss between augmented versions\n",
    "            contrastive_loss = self._contrastive_loss(latent_aug1, latent_aug2)\n",
    "            \n",
    "            # Total loss (weighted sum)\n",
    "            loss = recon_loss + 0.5 * contrastive_loss\n",
    "            \n",
    "            # Backpropagation\n",
    "            self.optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            self.optimizer.step()\n",
    "            \n",
    "            # Track losses\n",
    "            total_loss += loss.item()\n",
    "            total_recon_loss += recon_loss.item()\n",
    "            total_contrastive_loss += contrastive_loss.item()\n",
    "            \n",
    "            if batch_idx % 10 == 0:\n",
    "                print(f\"Epoch {epoch} [{batch_idx * len(data)}/{len(dataloader.dataset)} \"\n",
    "                      f\"({100. * batch_idx / len(dataloader):.0f}%)]\", end=' ')\n",
    "                print(f\"Loss: {loss.item():.6f} \"\n",
    "                      f\"Recon: {recon_loss.item():.6f} \"\n",
    "                      f\"Contrastive: {contrastive_loss.item():.6f}\")\n",
    "        \n",
    "        # Update scheduler if provided\n",
    "        if self.scheduler is not None:\n",
    "            self.scheduler.step()\n",
    "        \n",
    "        avg_loss = total_loss / len(dataloader)\n",
    "        avg_recon_loss = total_recon_loss / len(dataloader)\n",
    "        avg_contrastive_loss = total_contrastive_loss / len(dataloader)\n",
    "        \n",
    "        return avg_loss, avg_recon_loss, avg_contrastive_loss\n",
    "    \n",
    "    def _contrastive_loss(self, z1, z2):\n",
    "        \"\"\"\n",
    "        Contrastive loss function (SimCLR)\n",
    "        \"\"\"\n",
    "        batch_size = z1.size(0)\n",
    "        z1 = nn.functional.normalize(z1, dim=1)\n",
    "        z2 = nn.functional.normalize(z2, dim=1)\n",
    "        \n",
    "        # Full similarity matrix\n",
    "        z = torch.cat([z1, z2], dim=0)\n",
    "        sim_matrix = torch.mm(z, z.t()) / self.temperature\n",
    "        \n",
    "        # Mask for positive pairs\n",
    "        sim_matrix.fill_diagonal_(float('-inf'))\n",
    "        mask = torch.zeros_like(sim_matrix)\n",
    "        mask[:batch_size, batch_size:] = torch.eye(batch_size)\n",
    "        mask[batch_size:, :batch_size] = torch.eye(batch_size)\n",
    "        \n",
    "        # Compute loss\n",
    "        positives = torch.sum(torch.exp(sim_matrix) * mask, dim=1)\n",
    "        negatives = torch.sum(torch.exp(sim_matrix), dim=1) - positives\n",
    "        loss = -torch.log(positives / negatives).mean()\n",
    "        \n",
    "        return loss\n",
    "    \n",
    "    def _augment(self, data):\n",
    "        \"\"\"\n",
    "        Apply random augmentations to data\n",
    "        \"\"\"\n",
    "        batch_size, channels, time_steps = data.shape\n",
    "        augmented = data.clone()\n",
    "        \n",
    "        # Random noise\n",
    "        if torch.rand(1).item() < 0.5:\n",
    "            noise = torch.randn_like(augmented) * 0.05\n",
    "            augmented += noise\n",
    "        \n",
    "        # Random scaling\n",
    "        if torch.rand(1).item() < 0.5:\n",
    "            scale = torch.randn(batch_size, channels, 1).to(self.device) * 0.1 + 1.0\n",
    "            augmented *= scale\n",
    "        \n",
    "        # Random time masking\n",
    "        if torch.rand(1).item() < 0.5:\n",
    "            for i in range(batch_size):\n",
    "                mask_len = int(time_steps * (torch.rand(1).item() * 0.2))\n",
    "                start = int(torch.rand(1).item() * (time_steps - mask_len))\n",
    "                augmented[i, :, start:start+mask_len] = 0\n",
    "        \n",
    "        return augmented\n",
    "    \n",
    "    def evaluate(self, dataloader):\n",
    "        \"\"\"\n",
    "        Evaluate model on validation data\n",
    "        \"\"\"\n",
    "        self.model.eval()\n",
    "        total_loss = 0\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            for data, _, _ in dataloader:\n",
    "                data = data.to(self.device)\n",
    "                latent, reconstruction = self.model(data)\n",
    "                loss = self.reconstruction_loss(reconstruction, data)\n",
    "                total_loss += loss.item()\n",
    "        \n",
    "        avg_loss = total_loss / len(dataloader)\n",
    "        return avg_loss\n",
    "    \n",
    "    def get_embeddings(self, dataloader):\n",
    "        \"\"\"\n",
    "        Get latent embeddings for all data points\n",
    "        \"\"\"\n",
    "        self.model.eval()\n",
    "        embeddings = []\n",
    "        timestamps = []\n",
    "        signal_types = []\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            for data, signal_name, start_time in dataloader:\n",
    "                data = data.to(self.device)\n",
    "                latent = self.model.encode(data)\n",
    "                embeddings.append(latent.cpu().numpy())\n",
    "                timestamps.append(start_time.numpy())\n",
    "                signal_types.extend(signal_name)\n",
    "        \n",
    "        embeddings = np.vstack(embeddings)\n",
    "        timestamps = np.concatenate(timestamps)\n",
    "        \n",
    "        return embeddings, timestamps, signal_types"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26462591",
   "metadata": {},
   "source": [
    "## 6. Micro-arousal Detection and Clustering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "dcef92ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MicroArousalDetector:\n",
    "    def __init__(self, embeddings, timestamps, signal_types, min_samples=5, eps=0.5):\n",
    "        \"\"\"\n",
    "        Detect micro-arousals using clustering of embeddings\n",
    "        \n",
    "        Args:\n",
    "            embeddings: Latent embeddings from model\n",
    "            timestamps: Timestamps for each embedding\n",
    "            signal_types: Signal type for each embedding\n",
    "            min_samples: Minimum samples for DBSCAN clustering\n",
    "            eps: Maximum distance between samples for DBSCAN\n",
    "        \"\"\"\n",
    "        self.embeddings = embeddings\n",
    "        self.timestamps = timestamps\n",
    "        self.signal_types = signal_types\n",
    "        self.min_samples = min_samples\n",
    "        self.eps = eps\n",
    "        self.clusters = None\n",
    "    \n",
    "    def detect_clusters(self, method=\"dbscan\", n_clusters=5):\n",
    "        \"\"\"\n",
    "        Detect clusters in embedding space\n",
    "        \n",
    "        Args:\n",
    "            method: Clustering method (\"dbscan\" or \"kmeans\")\n",
    "            n_clusters: Number of clusters for K-means\n",
    "        \"\"\"\n",
    "        # Scale embeddings\n",
    "        scaler = StandardScaler()\n",
    "        scaled_embeddings = scaler.fit_transform(self.embeddings)\n",
    "        \n",
    "        if method == \"dbscan\":\n",
    "            # DBSCAN for density-based clustering\n",
    "            clustering = DBSCAN(eps=self.eps, min_samples=self.min_samples, n_jobs=-1)\n",
    "            clusters = clustering.fit_predict(scaled_embeddings)\n",
    "        else:\n",
    "            # K-means for fixed number of clusters\n",
    "            clustering = KMeans(n_clusters=n_clusters, random_state=42, n_init=10)\n",
    "            clusters = clustering.fit_predict(scaled_embeddings)\n",
    "        \n",
    "        self.clusters = clusters\n",
    "        return clusters\n",
    "    \n",
    "    def detect_micro_arousals(self, window_size=5, threshold=0.8):\n",
    "        \"\"\"\n",
    "        Detect micro-arousals based on cluster transitions\n",
    "        \n",
    "        Args:\n",
    "            window_size: Size of sliding window in seconds\n",
    "            threshold: Threshold for cluster transition ratio\n",
    "        \"\"\"\n",
    "        if self.clusters is None:\n",
    "            raise ValueError(\"Run detect_clusters first\")\n",
    "        \n",
    "        # Sort by timestamps\n",
    "        sorted_indices = np.argsort(self.timestamps)\n",
    "        sorted_timestamps = self.timestamps[sorted_indices]\n",
    "        sorted_clusters = self.clusters[sorted_indices]\n",
    "        \n",
    "        micro_arousals = []\n",
    "        \n",
    "        # Slide window and detect transitions\n",
    "        for i in range(0, len(sorted_timestamps) - window_size, 1):\n",
    "            window_clusters = sorted_clusters[i:i+window_size]\n",
    "            \n",
    "            # Count transitions\n",
    "            transitions = sum(window_clusters[j] != window_clusters[j+1] for j in range(len(window_clusters)-1))\n",
    "            transition_ratio = transitions / (len(window_clusters) - 1)\n",
    "            \n",
    "            if transition_ratio >= threshold:\n",
    "                micro_arousals.append({\n",
    "                    \"start_time\": sorted_timestamps[i],\n",
    "                    \"end_time\": sorted_timestamps[i+window_size-1],\n",
    "                    \"transition_ratio\": transition_ratio,\n",
    "                    \"clusters\": window_clusters.copy()\n",
    "                })\n",
    "        \n",
    "        return micro_arousals\n",
    "    \n",
    "    def visualize_clusters(self):\n",
    "        \"\"\"\n",
    "        Visualize clusters using t-SNE\n",
    "        \"\"\"\n",
    "        if self.clusters is None:\n",
    "            raise ValueError(\"Run detect_clusters first\")\n",
    "        \n",
    "        # Compute t-SNE embedding\n",
    "        tsne = TSNE(n_components=2, random_state=42, perplexity=30, n_iter=1000)\n",
    "        embedded = tsne.fit_transform(self.embeddings)\n",
    "        \n",
    "        # Plot clusters\n",
    "        plt.figure(figsize=(12, 10))\n",
    "        scatter = plt.scatter(embedded[:, 0], embedded[:, 1], c=self.clusters, cmap='viridis', alpha=0.7, s=10)\n",
    "        plt.colorbar(scatter)\n",
    "        plt.title(\"t-SNE Visualization of Sleep Stage Clusters\")\n",
    "        plt.xlabel(\"t-SNE Component 1\")\n",
    "        plt.ylabel(\"t-SNE Component 2\")\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "        \n",
    "        # Plot time series of clusters\n",
    "        sorted_indices = np.argsort(self.timestamps)\n",
    "        plt.figure(figsize=(14, 6))\n",
    "        plt.scatter(self.timestamps[sorted_indices], self.clusters[sorted_indices], \n",
    "                   c=self.clusters[sorted_indices], cmap='viridis', alpha=0.7, s=10)\n",
    "        plt.title(\"Cluster Assignment Over Time\")\n",
    "        plt.xlabel(\"Time (seconds)\")\n",
    "        plt.ylabel(\"Cluster\")\n",
    "        plt.colorbar(label=\"Cluster\")\n",
    "        plt.tight_layout()\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6296098e",
   "metadata": {},
   "source": [
    "## 7. Main Pipeline Implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "70026581",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_sleep_clustering_pipeline(edf_files, output_dir=\"./results\", epochs=50, batch_size=64):\n",
    "    \"\"\"\n",
    "    Run the complete sleep clustering pipeline\n",
    "    \n",
    "    Args:\n",
    "        edf_files: List of paths to EDF files\n",
    "        output_dir: Directory to save results\n",
    "        epochs: Number of training epochs\n",
    "        batch_size: Batch size for training\n",
    "    \"\"\"\n",
    "    # Create output directory\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "    \n",
    "    # 1. Load and preprocess data\n",
    "    print(\"Loading and preprocessing data...\")\n",
    "    dataset = PSGDataset(edf_files, window_size=30, overlap=0.5)\n",
    "    dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=True, num_workers=0, pin_memory=False)\n",
    "    \n",
    "    # 2. Initialize model and trainer\n",
    "    print(\"Initializing model...\")\n",
    "    model = CNN_LSTM(input_channels=7, hidden_dims=128, latent_dim=64).to(device)\n",
    "    optimizer = optim.Adam(model.parameters(), lr=1e-3, weight_decay=1e-5)\n",
    "    scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, 'min', patience=5, factor=0.5)\n",
    "    trainer = UnsupervisedTrainer(model, optimizer, scheduler, device)\n",
    "    \n",
    "    # 3. Train model\n",
    "    print(\"Starting training...\")\n",
    "    train_losses = []\n",
    "    recon_losses = []\n",
    "    contrastive_losses = []\n",
    "    \n",
    "    for epoch in range(1, epochs+1):\n",
    "        loss, recon_loss, contrastive_loss = trainer.train_epoch(dataloader, epoch)\n",
    "        train_losses.append(loss)\n",
    "        recon_losses.append(recon_loss)\n",
    "        contrastive_losses.append(contrastive_loss)\n",
    "        \n",
    "        print(f\"Epoch {epoch}/{epochs} - Loss: {loss:.6f}, \"\n",
    "              f\"Recon Loss: {recon_loss:.6f}, Contrastive Loss: {contrastive_loss:.6f}\")\n",
    "    \n",
    "    # Save model\n",
    "    torch.save(model.state_dict(), f\"{output_dir}/sleep_clustering_model.pt\")\n",
    "    \n",
    "    # Plot training curves\n",
    "    plt.figure(figsize=(10, 5))\n",
    "    plt.plot(train_losses, label='Total Loss')\n",
    "    plt.plot(recon_losses, label='Reconstruction Loss')\n",
    "    plt.plot(contrastive_losses, label='Contrastive Loss')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.legend()\n",
    "    plt.title('Training Losses')\n",
    "    plt.savefig(f\"{output_dir}/training_curves.png\")\n",
    "    plt.show()\n",
    "    \n",
    "    # 4. Get embeddings for all data\n",
    "    print(\"Extracting embeddings...\")\n",
    "    eval_dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=False, num_workers=4, pin_memory=True)\n",
    "    embeddings, timestamps, signal_types = trainer.get_embeddings(eval_dataloader)\n",
    "    \n",
    "    # 5. Apply DMD for additional feature extraction\n",
    "    print(\"Applying DMD for feature extraction...\")\n",
    "    dmd = DMD(rank=20)\n",
    "    dmd_features = dmd.fit_transform(embeddings.T)  # Transpose for DMD\n",
    "    \n",
    "    # Combine original embeddings with DMD features\n",
    "    combined_features = np.hstack([embeddings, dmd_features])\n",
    "    \n",
    "    # 6. Detect micro-arousals\n",
    "    print(\"Detecting micro-arousals...\")\n",
    "    detector = MicroArousalDetector(combined_features, np.array(timestamps), signal_types)\n",
    "    \n",
    "    # Try different clustering methods\n",
    "    print(\"Detecting clusters with DBSCAN...\")\n",
    "    detector.detect_clusters(method=\"dbscan\")\n",
    "    detector.visualize_clusters()\n",
    "    \n",
    "    print(\"Detecting clusters with K-means...\")\n",
    "    detector.detect_clusters(method=\"kmeans\", n_clusters=5)\n",
    "    detector.visualize_clusters()\n",
    "    \n",
    "    # Find micro-arousals\n",
    "    micro_arousals = detector.detect_micro_arousals(window_size=5, threshold=0.7)\n",
    "    \n",
    "    print(f\"Detected {len(micro_arousals)} potential micro-arousals\")\n",
    "    \n",
    "    # Save results\n",
    "    with open(f\"{output_dir}/micro_arousals.txt\", \"w\") as f:\n",
    "        for i, arousal in enumerate(micro_arousals):\n",
    "            f.write(f\"Micro-arousal {i+1}: {arousal['start_time']:.2f}s to {arousal['end_time']:.2f}s \"\n",
    "                   f\"(Transition ratio: {arousal['transition_ratio']:.2f})\\n\")\n",
    "    \n",
    "    # Return results for further analysis\n",
    "    return {\n",
    "        \"model\": model,\n",
    "        \"embeddings\": embeddings,\n",
    "        \"dmd_features\": dmd_features,\n",
    "        \"combined_features\": combined_features,\n",
    "        \"micro_arousals\": micro_arousals,\n",
    "        \"detector\": detector\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "50f081c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def custom_collate(batch):\n",
    "    \"\"\"\n",
    "    Custom collate function to handle variable-sized tensors\n",
    "    \"\"\"\n",
    "    # Extract data, signal_names, and start_times\n",
    "    data = [item[0] for item in batch]\n",
    "    signal_names = [item[1] for item in batch]\n",
    "    start_times = [item[2] for item in batch]\n",
    "    \n",
    "    # Handle data tensors of different sizes\n",
    "    if isinstance(data[0], torch.Tensor):\n",
    "        # Option 1: Pad all tensors to the maximum size\n",
    "        max_length = max([d.size(0) for d in data])\n",
    "        data_padded = []\n",
    "        \n",
    "        for d in data:\n",
    "            if d.size(0) < max_length:\n",
    "                # Create padding\n",
    "                padding = torch.zeros(max_length - d.size(0), *d.size()[1:], device=d.device)\n",
    "                # Concatenate with original data\n",
    "                d_padded = torch.cat([d, padding], dim=0)\n",
    "                data_padded.append(d_padded)\n",
    "            else:\n",
    "                data_padded.append(d)\n",
    "        \n",
    "        # Stack the padded tensors\n",
    "        data_tensor = torch.stack(data_padded)\n",
    "    else:\n",
    "        # If not tensors, just keep as a list\n",
    "        data_tensor = data\n",
    "    \n",
    "    # Convert start_times to tensor if they're numeric\n",
    "    try:\n",
    "        start_times_tensor = torch.tensor(start_times)\n",
    "    except:\n",
    "        start_times_tensor = start_times\n",
    "    \n",
    "    return data_tensor, signal_names, start_times_tensor"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a51a076",
   "metadata": {},
   "source": [
    "## 8. Example Usage\n",
    "\n",
    "Assuming you have EDF files containing PSG data, you can use the pipeline as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "a5d8cd93",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading and preprocessing data...\n",
      "Initializing model...\n",
      "Starting training...\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "stack expects each tensor to be equal size, but got [3000] at entry 0 and [30] at entry 1",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[11], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# Example usage (replace with your actual EDF file paths)\u001b[39;00m\n\u001b[1;32m      2\u001b[0m edf_files \u001b[38;5;241m=\u001b[39m [\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mraw data/SC4001E0-PSG.edf\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[0;32m----> 3\u001b[0m results \u001b[38;5;241m=\u001b[39m \u001b[43mrun_sleep_clustering_pipeline\u001b[49m\u001b[43m(\u001b[49m\u001b[43medf_files\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moutput_dir\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m./sleep_results\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m50\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m      5\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPipeline ready to use. Please provide paths to your EDF files containing PSG data to begin.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m      6\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mExample: results = run_sleep_clustering_pipeline([\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m/path/to/psg1.edf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m, \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m/path/to/psg2.edf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m])\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "Cell \u001b[0;32mIn[7], line 33\u001b[0m, in \u001b[0;36mrun_sleep_clustering_pipeline\u001b[0;34m(edf_files, output_dir, epochs, batch_size)\u001b[0m\n\u001b[1;32m     30\u001b[0m contrastive_losses \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m     32\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m epoch \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m1\u001b[39m, epochs\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m1\u001b[39m):\n\u001b[0;32m---> 33\u001b[0m     loss, recon_loss, contrastive_loss \u001b[38;5;241m=\u001b[39m \u001b[43mtrainer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain_epoch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdataloader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepoch\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     34\u001b[0m     train_losses\u001b[38;5;241m.\u001b[39mappend(loss)\n\u001b[1;32m     35\u001b[0m     recon_losses\u001b[38;5;241m.\u001b[39mappend(recon_loss)\n",
      "Cell \u001b[0;32mIn[5], line 27\u001b[0m, in \u001b[0;36mUnsupervisedTrainer.train_epoch\u001b[0;34m(self, dataloader, epoch)\u001b[0m\n\u001b[1;32m     24\u001b[0m total_recon_loss \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[1;32m     25\u001b[0m total_contrastive_loss \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[0;32m---> 27\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m batch_idx, (data, signal_names, _) \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(dataloader):\n\u001b[1;32m     28\u001b[0m     data \u001b[38;5;241m=\u001b[39m data\u001b[38;5;241m.\u001b[39mto(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdevice)\n\u001b[1;32m     30\u001b[0m     \u001b[38;5;66;03m# Create augmented versions\u001b[39;00m\n",
      "File \u001b[0;32m~/notebooks/.conda/lib/python3.10/site-packages/torch/utils/data/dataloader.py:630\u001b[0m, in \u001b[0;36m_BaseDataLoaderIter.__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    627\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sampler_iter \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    628\u001b[0m     \u001b[38;5;66;03m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[1;32m    629\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reset()  \u001b[38;5;66;03m# type: ignore[call-arg]\u001b[39;00m\n\u001b[0;32m--> 630\u001b[0m data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_next_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    631\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m    632\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dataset_kind \u001b[38;5;241m==\u001b[39m _DatasetKind\u001b[38;5;241m.\u001b[39mIterable \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[1;32m    633\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[1;32m    634\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m>\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called:\n",
      "File \u001b[0;32m~/notebooks/.conda/lib/python3.10/site-packages/torch/utils/data/dataloader.py:673\u001b[0m, in \u001b[0;36m_SingleProcessDataLoaderIter._next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    671\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21m_next_data\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    672\u001b[0m     index \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_next_index()  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[0;32m--> 673\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_dataset_fetcher\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfetch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mindex\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[1;32m    674\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory:\n\u001b[1;32m    675\u001b[0m         data \u001b[38;5;241m=\u001b[39m _utils\u001b[38;5;241m.\u001b[39mpin_memory\u001b[38;5;241m.\u001b[39mpin_memory(data, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory_device)\n",
      "File \u001b[0;32m~/notebooks/.conda/lib/python3.10/site-packages/torch/utils/data/_utils/fetch.py:55\u001b[0m, in \u001b[0;36m_MapDatasetFetcher.fetch\u001b[0;34m(self, possibly_batched_index)\u001b[0m\n\u001b[1;32m     53\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     54\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[possibly_batched_index]\n\u001b[0;32m---> 55\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcollate_fn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/notebooks/.conda/lib/python3.10/site-packages/torch/utils/data/_utils/collate.py:317\u001b[0m, in \u001b[0;36mdefault_collate\u001b[0;34m(batch)\u001b[0m\n\u001b[1;32m    256\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mdefault_collate\u001b[39m(batch):\n\u001b[1;32m    257\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    258\u001b[0m \u001b[38;5;124;03m    Take in a batch of data and put the elements within the batch into a tensor with an additional outer dimension - batch size.\u001b[39;00m\n\u001b[1;32m    259\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    315\u001b[0m \u001b[38;5;124;03m        >>> default_collate(batch)  # Handle `CustomType` automatically\u001b[39;00m\n\u001b[1;32m    316\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 317\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mcollate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcollate_fn_map\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdefault_collate_fn_map\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/notebooks/.conda/lib/python3.10/site-packages/torch/utils/data/_utils/collate.py:174\u001b[0m, in \u001b[0;36mcollate\u001b[0;34m(batch, collate_fn_map)\u001b[0m\n\u001b[1;32m    171\u001b[0m transposed \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(\u001b[38;5;28mzip\u001b[39m(\u001b[38;5;241m*\u001b[39mbatch))  \u001b[38;5;66;03m# It may be accessed twice, so we use a list.\u001b[39;00m\n\u001b[1;32m    173\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(elem, \u001b[38;5;28mtuple\u001b[39m):\n\u001b[0;32m--> 174\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m [collate(samples, collate_fn_map\u001b[38;5;241m=\u001b[39mcollate_fn_map) \u001b[38;5;28;01mfor\u001b[39;00m samples \u001b[38;5;129;01min\u001b[39;00m transposed]  \u001b[38;5;66;03m# Backwards compatibility.\u001b[39;00m\n\u001b[1;32m    175\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    176\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n",
      "File \u001b[0;32m~/notebooks/.conda/lib/python3.10/site-packages/torch/utils/data/_utils/collate.py:174\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    171\u001b[0m transposed \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(\u001b[38;5;28mzip\u001b[39m(\u001b[38;5;241m*\u001b[39mbatch))  \u001b[38;5;66;03m# It may be accessed twice, so we use a list.\u001b[39;00m\n\u001b[1;32m    173\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(elem, \u001b[38;5;28mtuple\u001b[39m):\n\u001b[0;32m--> 174\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m [\u001b[43mcollate\u001b[49m\u001b[43m(\u001b[49m\u001b[43msamples\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcollate_fn_map\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcollate_fn_map\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m samples \u001b[38;5;129;01min\u001b[39;00m transposed]  \u001b[38;5;66;03m# Backwards compatibility.\u001b[39;00m\n\u001b[1;32m    175\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    176\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n",
      "File \u001b[0;32m~/notebooks/.conda/lib/python3.10/site-packages/torch/utils/data/_utils/collate.py:142\u001b[0m, in \u001b[0;36mcollate\u001b[0;34m(batch, collate_fn_map)\u001b[0m\n\u001b[1;32m    140\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m collate_fn_map \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    141\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m elem_type \u001b[38;5;129;01min\u001b[39;00m collate_fn_map:\n\u001b[0;32m--> 142\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mcollate_fn_map\u001b[49m\u001b[43m[\u001b[49m\u001b[43melem_type\u001b[49m\u001b[43m]\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcollate_fn_map\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcollate_fn_map\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    144\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m collate_type \u001b[38;5;129;01min\u001b[39;00m collate_fn_map:\n\u001b[1;32m    145\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(elem, collate_type):\n",
      "File \u001b[0;32m~/notebooks/.conda/lib/python3.10/site-packages/torch/utils/data/_utils/collate.py:214\u001b[0m, in \u001b[0;36mcollate_tensor_fn\u001b[0;34m(batch, collate_fn_map)\u001b[0m\n\u001b[1;32m    212\u001b[0m     storage \u001b[38;5;241m=\u001b[39m elem\u001b[38;5;241m.\u001b[39m_typed_storage()\u001b[38;5;241m.\u001b[39m_new_shared(numel, device\u001b[38;5;241m=\u001b[39melem\u001b[38;5;241m.\u001b[39mdevice)\n\u001b[1;32m    213\u001b[0m     out \u001b[38;5;241m=\u001b[39m elem\u001b[38;5;241m.\u001b[39mnew(storage)\u001b[38;5;241m.\u001b[39mresize_(\u001b[38;5;28mlen\u001b[39m(batch), \u001b[38;5;241m*\u001b[39m\u001b[38;5;28mlist\u001b[39m(elem\u001b[38;5;241m.\u001b[39msize()))\n\u001b[0;32m--> 214\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstack\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mout\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: stack expects each tensor to be equal size, but got [3000] at entry 0 and [30] at entry 1"
     ]
    }
   ],
   "source": [
    "# Example usage (replace with your actual EDF file paths)\n",
    "edf_files = [\"raw data/SC4001E0-PSG.edf\"]\n",
    "results = run_sleep_clustering_pipeline(edf_files, output_dir=\"./sleep_results\", epochs=50)\n",
    "\n",
    "print(\"Pipeline ready to use. Please provide paths to your EDF files containing PSG data to begin.\")\n",
    "print(\"Example: results = run_sleep_clustering_pipeline(['/path/to/psg1.edf', '/path/to/psg2.edf'])\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b475c53a",
   "metadata": {},
   "source": [
    "## 9. Conclusion\n",
    "\n",
    "This pipeline implements an unsupervised deep learning approach for analyzing PSG signals and identifying micro-arousals at a millisecond scale. The key components are:\n",
    "\n",
    "1. Data preprocessing of EDF files containing PSG signals\n",
    "2. Feature extraction using Dynamic Mode Decomposition (DMD)\n",
    "3. A CNN+LSTM architecture for temporal sequence modeling\n",
    "4. Unsupervised learning with reconstruction and contrastive losses\n",
    "5. Clustering and visualization of sleep stages\n",
    "6. Detection of micro-arousals based on cluster transitions\n",
    "\n",
    "This approach allows for the discovery of new patterns in sleep data without relying on labeled data, which can help researchers better understand sleep disorders and patterns."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
